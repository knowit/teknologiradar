"name","ring","quadrant","isNew","description"
Dependency drift fitness function,Adopt,Techniques,FALSE,"<p>Fitness functions introduced by <a href=""/radar/Techniques/evolutionary-architecture"">evolutionary architecture</a>, borrowed from <a href=""https://en.wikipedia.org/wiki/Evolutionary_computation#:%7E:text=In%20computer%20science%2C%20evolutionary%20computation,soft%20computing%20studying%20these%20algorithms."">evolutionary computing</a>, are executable functions that inform us if our applications and architecture are objectively moving away from their desired characteristics. They're essentially tests that can be incorporated into our release pipelines. One of the major characteristics of an application is the freshness of its dependencies to other libraries, APIs or environmental components that a <strong>dependency drift fitness function</strong> tracks to flag the out-of-date dependencies that require updating. With the growing and maturing number of tools that detect dependency drifts, such as <a href=""/radar/tools/dependabot"">Dependabot</a> or <a href=""/radar/tools/snyk"">Snyk</a>, we can easily incorporate dependency drift fitness functions into our software release process to take timely action in keeping our application dependencies up to date.</p>"
Run cost as architecture fitness function,Adopt,Techniques,FALSE,"<p>Automating the estimation, tracking and projection of cloud infrastructure's run cost is necessary for today's organizations. The cloud providers' savvy pricing models, combined with the proliferation of pricing parameters and the dynamic nature of today's architecture, can lead to surprisingly expensive run costs. For example, the price of <a href=""/radar/Techniques/serverless-architecture"">serverless</a> based on API calls, event streaming solutions based on traffic or data processing clusters based on running jobs, all have a dynamic nature that changes over time as the architecture evolves. When our teams manage infrastructure on the cloud, implementing <strong>run cost as architecture fitness function</strong> is one of their early activities. This means that our teams can observe the cost of running services against the value delivered; when they see deviations from what was expected or acceptable, they'll discuss whether it's time to evolve the architecture. The observation and calculation of the run cost is implemented as an automated function.</p>"
Security policy as code,Adopt,Techniques,FALSE,"<p>As the technology landscape is becoming more complex, concerns such as security need more automation and engineering practices. When building systems, we need to take into consideration security policies, which are rules and procedures to protect our systems from threats and disruption. For example, access control policies define and enforce who can access which services and resources under what circumstances; by contrast, network security policies can dynamically limit the traffic rate to a particular service.</p>

<p>Several of our teams have had a great experience treating <strong>security policy as code</strong>. When we say <em>as code</em>, we not only mean to write these security policies in a file but also to apply practices such as keeping the code under version control, introducing automatic validation in the pipeline, automatically deploying them in the environments and observing and monitoring their performance. Based on our experience and the maturity of the existing tools — including <a href=""/radar/tools/open-policy-agent-opa"">Open Policy Agent</a> and platforms such as <a href=""/radar/platforms/istio"">Istio</a> which provide flexible policy definition and enforcement mechanisms that support the practice of security policy as code — we highly recommend using this technique in your environment.</p>"
Tailored service templates,Adopt,Techniques,FALSE,"<p>Since we last mentioned <strong>tailored service templates</strong>, we've seen a broader adoption of the pattern to help pave the road for organizations moving to microservices. With constant advances in observability tooling, container orchestration and service mesh sidecars, a template provides sensible defaults to bootstrap a new service, removing a great deal of setup needed to make the service work well with the surrounding infrastructure. We've had success <a href=""/radar/Techniques/applying-product-management-to-internal-platforms"">applying product management</a> principles to tailored service templates, treating internal developers as customers and making it easier for them to push code to production and operate it with appropriate observability. This has the added benefit of acting as a lightweight governance mechanism to centralize default technical decisions.</p>"
Continuous delivery for machine learning (CD4ML),Trial,Techniques,FALSE,"<p>About a decade ago we introduced <a href=""/radar/Techniques/continuous-delivery-cd"">continuous delivery (CD)</a>, our default way to deliver software solutions. Today's solutions increasingly include machine-learning models and we find them no exception in adopting continuous delivery practices. We call this <strong><a href=""https://martinfowler.com/articles/cd4ml.html"">continuous delivery for machine learning (CD4ML)</a></strong>. Although the principles of CD remain the same, the practices and tools to implement the end-to-end process of training, testing, deploying and monitoring models require some modifications. For example: version control must not only include code but also the data, the models and its parameters; the testing pyramid extends to include model bias, fairness and data and feature validation; the deployment process must consider how to promote and evaluate the performance of new models against current champion models. While the industry is celebrating the new buzzword of MLOps, we feel CD4ML is our holistic approach to implement an end-to-end process to reliably release and continuously improve machine-learning models, from idea to production.</p>"
Data mesh,Trial,Techniques,FALSE,"<p><strong><a href=""https://martinfowler.com/articles/data-monolith-to-mesh.html"">Data mesh</a></strong> marks a welcome architectural and organizational paradigm shift in how we manage big analytical data. The paradigm is founded on four principles: (1) domain-oriented decentralization of data ownership and architecture; (2) domain-oriented data served as a product; (3) self-serve data infrastructure as a platform to enable autonomous, domain-oriented data teams; and (4) federated governance to enable ecosystems and interoperability. Although the principles are intuitive and attempt to address many of the known challenges of previous centralized analytical data management, they transcend the available analytical data technologies. After building data mesh for multiple clients on top of the existing tooling, we learned two things: (a) there is a large gap in open-source or commercial tooling to accelerate implementation of data mesh (for example, implementation of a universal access model to time-based polyglot data which we currently custom build for our clients) and (b) despite the gap, it's feasible to use the existing technologies as the basic building blocks.</p>

<p>Naturally, technology fit is a major component of implementing your organization's data strategy based on data mesh. Success, however, demands an organizational restructure to separate the data platform team, create the role of data product owner for each domain and introduce the incentive structures necessary for domains to own and share their analytical data as products.</p>"
Declarative data pipeline definition,Trial,Techniques,FALSE,"<p>Many data pipelines are defined in a large, more or less imperative script written in Python or Scala. The script contains the logic of the individual steps as well as the code chaining the steps together. When faced with a similar situation in Selenium tests, developers discovered the Page Object pattern, and later many behavior-driven development (BDD) frameworks implemented a split between step definitions and their composition. Some teams are now experimenting with bringing the same thinking to data engineering. A separate <strong>declarative data pipeline definition</strong>, maybe written in YAML, contains only the declaration and sequence of steps. It states input and output data sets but refers to scripts if and when more complex logic is needed. <a href=""https://github.com/binaryaffairs/a-la-mode"">A La Mode</a> is a relatively new tool that takes a DSL approach to defining pipelines, but <a href=""https://github.com/rambler-digital-solutions/airflow-declarative"">airflow-declarative</a>, a tool that turns directed acyclic graphs defined in YAML into <a href=""/radar/tools/airflow"">Airflow</a> task schedules, seems to have the most momentum in this space.</p>"
Diagrams as code,Trial,Techniques,TRUE,"<p>We're seeing more and more tools that enable you to create software architecture and other <strong>diagrams as code</strong>. There are benefits to using these tools over the heavier alternatives, including easy version control and the ability to generate the DSLs from many sources. Tools in this space that we like include <a href=""https://diagrams.mingrammer.com/"">Diagrams</a>, <a href=""https://structurizr.com/dsl"">Structurizr DSL</a>, <a href=""https://asciidoctor.org/docs/asciidoctor-diagram/"">AsciiDoctor Diagram</a> and stables such as <a href=""https://www.websequencediagrams.com/"">WebSequenceDiagrams</a>, <a href=""/radar/tools/plantuml"">PlantUML</a> and the venerable <a href=""https://graphviz.org/"">Graphviz</a>. It's also fairly simple to generate your own SVG these days, so don't rule out quickly writing your own tool either. One of our authors wrote a small <a href=""/radar/languages-and-frameworks/ruby"">Ruby</a> script to quickly create SVGs, for example.</p>"
Distroless Docker images,Trial,Techniques,FALSE,"<p>When building <a href=""/radar/platforms/docker"">Docker</a> images for our applications, we're often concerned with two things: the security and the size of the image. Traditionally, we've used <a href=""/radar/Techniques/container-security-scanning"">container security scanning</a> tools to detect and patch <a href=""https://cve.mitre.org/"">common vulnerabilities and exposures</a> and small distributions such as <a href=""https://alpinelinux.org/"">Alpine Linux</a> to address the image size and distribution performance. We've now gained more experience with <strong>distroless Docker images</strong> and are ready to recommend this approach as another important security precaution for containerized applications. Distroless Docker images reduce the footprint and dependencies by doing away with a full operating system distribution. This technique reduces security scan noise and the application attack surface. There are fewer vulnerabilities that need to be patched and as a bonus, these smaller images are more efficient. Google has published a set of <a href=""https://github.com/GoogleContainerTools/distroless"">distroless container images</a> for different languages. You can create distroless application images using the Google build tool <a href=""https://bazel.build/"">Bazel</a> or simply use multistage Dockerfiles. Note that distroless containers by default don't have a shell for debugging. However, you can easily find debug versions of distroless containers online, including a <a href=""https://busybox.net/downloads/BusyBox.html"">BusyBox shell</a>. Distroless Docker images is a technique pioneered by Google and, in our experience, is still largely confined to Google-generated images. We're hoping that the technique catches on beyond this ecosystem.</p>"
Event interception,Trial,Techniques,TRUE,"<p>As many more companies migrate away from their legacy systems, we feel it's worth highlighting an alternative to change data capture (CDC) as a mechanism for getting data from these systems. Martin Fowler described <strong><a href=""https://www.martinfowler.com/bliki/EventInterception.html"">event interception</a></strong> back in 2004. In modern terms it involves forking requests on ingress to a system so that it's possible to gradually build a replacement. Often this is done by copying events or messages but forking HTTP requests is equally valid. Examples include forking events from point-of-sale systems before they're written to a mainframe and forking payment transactions before they're written to a core banking system. Both lead to the gradual replacement of parts of the legacy systems. We feel that as a technique, obtaining state changes from the source, rather than trying to recreate them postprocessing using CDC, has been overlooked which is why we're highlighting it in this issue of the Radar.</p>"
Parallel run with reconciliation,Trial,Techniques,TRUE,"<p>Replacing legacy code at scale is always a difficult endeavor and one that often benefits from executing a <strong>parallel run with reconciliation</strong>. In practice, the technique relies on executing the same production flow through both the old and new code, returning the response from the legacy code but comparing the results to gain confidence in the new code. Despite being an old technique, we've seen more robust implementations in recent years building on continuous delivery practices such as canary releases and feature toggles and extending them by adding an extra layer of experimentation and data analysis to compare live results. We've even used the approach to compare cross-functional results such as response time. Although we've used the technique multiple times with bespoke tooling, we certainly owe a nod to GitHub's <a href=""https://github.com/github/scientist"">Scientist</a> tool, which they used to modernize a critical piece of their application and which has now been ported to multiple languages.</p>"
"Use ""remote native"" processes and approaches",Trial,Techniques,FALSE,"<p>As the pandemic stretches on it seems that highly distributed teams will be the ""new normal,"" at least for the time being. Over the past six months we've learnt a lot about effective remote working. On the positive side, good visual work-management and collaboration tools have made it easier than ever to collaborate remotely with colleagues. Developers, for example, can count on <a href=""/radar/tools/visual-studio-live-share"">Visual Studio Live Share</a> and <a href=""https://visualstudio.microsoft.com/services/github-codespaces/"">GitHub Codespaces</a> to facilitate teamwork and increase productivity. The biggest downside to remote work might be burnout: far too many people are scheduled for back-to-back video calls all day long, and this has begun to take its toll. While online visual tools make it easier to collaborate, it's also possible to build complex giant diagrams that end up being very hard to use, and the security aspects of tool proliferation also need to be carefully managed. Our advice is to remember to take a step back, talk to your teams, evaluate what's working and what's not and change processes and tools as needed.</p>"
Zero trust architecture,Trial,Techniques,FALSE,"<p>While the fabric of computing and data continues to shift in enterprises — from monolithic applications to <a href=""/radar/Techniques/microservices"">microservices</a>, from centralized data lakes to <a href=""/radar/Techniques/data-mesh"">data mesh</a>, from on-prem hosting to polycloud, with an increasing proliferation of connected devices — the approach to securing enterprise assets for the most part remains unchanged, with heavy reliance and trust in the network perimeter: Organizations continue to make heavy investments to secure their assets by hardening the virtual walls of their enterprises, using private links and firewall configurations and replacing static and cumbersome security processes that no longer serve the reality of today. This continuing trend compelled us to highlight <strong>zero trust architecture</strong> (ZTA) again.</p>

<p>ZTA is a paradigm shift in security architecture and strategy. It’s based on the assumption that a network perimeter is no longer representative of a secure boundary and no implicit trust should be granted to users or services based solely on their physical or network location. The number of resources, tools and platforms available to implement aspects of ZTA keeps growing and includes: enforcing <a href=""/radar/Techniques/security-policy-as-code"">policies as code</a> based on the least privilege and as granular as possible principles and continuous monitoring and automated mitigation of threats; using <a href=""/radar/Techniques/service-mesh"">service mesh</a> to enforce security control application-to-service and service-to-service; implementing <a href=""/radar/Techniques/binary-attestation"">binary attestation</a> to verify the origin of the binaries; and including <a href=""/radar/Techniques/secure-enclaves"">secure enclaves</a> in addition to traditional encryption to enforce the three pillars of data security: in transit, at rest and in memory. For introductions to the topic, consult the <a href=""https://csrc.nist.gov/publications/detail/sp/800-207/final"">NIST ZTA</a> publication and Google's white paper on <a href=""https://cloud.google.com/security/beyondprod"">BeyondProd</a>.</p>"
Bounded low-code platforms,Assess,Techniques,TRUE,"<p>One of the most nuanced decisions facing companies at the moment is the adoption of low-code or no-code platforms, that is, platforms that solve very specific problems in very limited domains. Many vendors are pushing aggressively into this space. The problems we see with these platforms typically relate to an inability to apply good engineering practices such as versioning. Testing too is typically really hard. However, we noticed some interesting new entrants to the market — including <a href=""https://www.honeycode.aws/"">Amazon Honeycode</a>, which makes it easy to create simple task or event management apps, and <a href=""https://parabola.io/"">Parabola</a> for IFTTT-like cloud workflows — which is why we're including <strong>bounded low-code platforms</strong> in this volume. Nevertheless, we remain deeply skeptical about their wider applicability since these tools, like Japanese Knotweed, have a knack of escaping their bounds and tangling everything together. That's why we still strongly advise caution in their adoption.</p>"
Browser-tailored polyfills,Assess,Techniques,TRUE,"<p>Polyfills are extremely useful to help the web evolve, providing substitute implementations of modern features for browsers that don't implement them (yet). Too often, though, web applications ship polyfills to browsers that don't need them, which causes unnecessary download and parsing overhead. The situation is becoming more pronounced now as only a few rendering engines remain and the bulk of the polyfills target only one of them: the Trident renderer in IE11. Further, market share of IE11 is dwindling with <a href=""https://techcommunity.microsoft.com/t5/microsoft-365-blog/microsoft-365-apps-say-farewell-to-internet-explorer-11-and/ba-p/1591666"">support ending</a> in less than a year. We therefore suggest that you make use of <strong>browser-tailored polyfills</strong>, shipping only necessary polyfills to a given browser. This technique can even be implemented as a service with <a href=""https://polyfill.io/v3/"">Polyfill.io</a>.</p>"
Decentralized identity,Assess,Techniques,FALSE,"<p>In 2016, Christopher Allen, a key contributor to <a href=""https://en.wikipedia.org/wiki/Transport_Layer_Security"">SSL/TLS</a>, inspired us with an introduction of 10 principles underpinning a new form of digital identity and a path to get there, <a href=""http://www.lifewithalacrity.com/2016/04/the-path-to-self-soverereign-identity.html"">the path to self-sovereign identity</a>. Self-sovereign identity, also known as <strong>decentralized identity</strong>, is a “lifetime portable identity for any person, organization, or thing that does not depend on any centralized authority and can never be taken away,” according to the <a href=""/radar/platforms/trust-over-ip-stack"">Trust over IP</a> standard. Adopting and implementing decentralized identity is gaining momentum and becoming attainable. We see its adoption in privacy-respecting <a href=""https://www.civic.com/healthkey/"">customer health applications</a>, <a href=""https://www.truu.id/"">government healthcare infrastructure</a> and <a href=""https://id-bulletin.com/2020/06/04/news-gleif-and-evernym-demo-organization-wallets-to-deliver-trust-and-transparency-in-digital-business/"">corporate legal identity</a>. If you want to rapidly get started with decentralized identity, you can assess <a href=""https://sovrin.org/"">Sovrin Network</a>, <a href=""https://github.com/hyperledger/aries"">Hyperledger Aries</a> and <a href=""https://github.com/hyperledger/indy-node"">Indy</a> OSS, as well as <a href=""https://www.w3.org/TR/did-core/"">decentralized identifiers</a> and <a href=""/radar/Techniques/verifiable-credentials"">verifiable credentials</a> standards. We're watching this space closely as we help our clients with their strategic positioning in the new era of digital trust.</p>"
Kube-managed cloud services,Assess,Techniques,TRUE,"<p>Cloud providers have slowly started supporting <a href=""/radar/platforms/kubernetes"">Kubernetes</a>-style APIs, via custom resource definitions (CRDs), for managing their cloud services. In most cases these cloud services are a core part of the infrastructure, and we've seen teams use tools such as <a href=""/radar/tools/terraform"">Terraform</a> or <a href=""/radar/platforms/pulumi"">Pulumi</a> to provision them. With these new CRDs (<a href=""https://github.com/aws/aws-controllers-k8s"">ACK</a> for <a href=""/radar/platforms/aws"">AWS</a>, <a href=""https://github.com/Azure/azure-service-operator"">Azure Service Operator</a> for <a href=""/radar/platforms/azure"">Azure</a> and <a href=""https://cloud.google.com/config-connector/docs/overview"">Config Connectors</a> for <a href=""/radar/platforms/google-cloud-platform"">GCP</a>) you can use Kubernetes to provision and manage these cloud services. One advantage of these <strong>Kube-managed cloud services</strong> is that you can leverage the same Kubernetes control plane to enforce the declarative state of both your application and infrastructure. The downside is that it tightly couples your Kubernetes cluster with infrastructure, so we're carefully assessing it and you should too.</p>"
Open Application Model (OAM),Assess,Techniques,TRUE,"<p>We've talked a lot about the benefits of creating <a href=""/radar/Techniques/platform-engineering-product-teams"">platform engineering product teams</a> in support of your other product teams, but actually doing it is hard. It seems that the industry is still searching for the right abstraction in the world of <a href=""/radar/Techniques/infrastructure-as-code"">infrastructure as code</a>. Although tools such as <a href=""/radar/tools/terraform"">Terraform</a> and <a href=""/radar/tools/helm"">Helm</a> are steps in the right direction, the focus is still on managing infrastructure as opposed to application development. There are also shifts toward the concept of infrastructure as software with new tools such as <a href=""/radar/platforms/pulumi"">Pulumi</a> and <a href=""/radar/platforms/aws-cloud-development-kit"">CDK</a> being released. The <strong><a href=""https://oam.dev/"">Open Application Model (OAM)</a></strong> is an attempt to bring some standardization to this space. Using the abstractions of components, application configurations, scopes and traits, developers can describe their applications in a platform-agnostic way, while platform implementers define their platform in terms of workload, trait and scope. Whether the OAM will be widely adopted remains to be seen, but we recommend keeping an eye on this interesting and needed idea.</p>"
Secure enclaves,Assess,Techniques,TRUE,"<p><strong>Secure enclaves</strong>, also identified as <a href=""https://en.wikipedia.org/wiki/Trusted_execution_environment"">trusted execution environments (TEE)</a>, refer to a technique that isolates an environment — processor, memory and storage — with a higher level of security and only provides a limited exchange of information with its surrounding untrusted execution context. For example, a secure enclave at the hardware and OS levels can create and store private keys and perform operations with them such as encrypt data or verify signatures without the private keys leaving the secure enclave or being loaded in the untrusted application memory. Secure enclave provides a limited set of instructions to perform trusted operations, isolated from an untrusted application context.</p>

<p>The technique has long been supported by many hardware and OS providers (including <a href=""https://support.apple.com/guide/security/secure-enclave-overview-sec59b0b31ff/web"">Apple</a>), and developers have used it in IoT and edge applications. Only recently, however, has it gained attention in enterprise and cloud-based applications. Cloud providers have started to introduce <a href=""https://confidentialcomputing.io/"">confidential computing</a> features such as hardware-based secure enclaves: <a href=""https://azure.microsoft.com/en-us/solutions/confidential-compute/"">Azure confidential computing infrastructure</a> promises TEE-enabled VMs and access through the <a href=""https://github.com/openenclave/openenclave"">Open Enclave SDK</a> open-source library to perform trusted operations. Similarly, <a href=""https://cloud.google.com/compute/confidential-vm/docs/about-cvm"">GCP Confidential VMs and Compute Engine</a>, still in beta, allow using VMs with data encryption in memory, and <a href=""https://aws.amazon.com/ec2/nitro/nitro-enclaves/"">AWS Nitro Enclaves</a> is following them with its upcoming preview release. With the introduction of cloud-based secure enclaves and confidential computing, we can add a third pillar to data protection: in rest, in transit and now in memory.</p>

<p>Even though we're still in the very early days of secure enclaves for enterprise, we encourage you to consider this technique, while staying informed about known <a href=""https://en.wikipedia.org/wiki/Foreshadow"">vulnerabilities</a> that can compromise the secure enclaves of the underlying hardware providers.</p>"
Switchback experimentation,Assess,Techniques,TRUE,"<p>Controlled experiments using A/B testing is a great way to inform decisions around product development. But it doesn't work well when we can't establish independence between the two groups involved in the A/B test — i.e., adding someone to the ""A"" group impacts the ""B"" group and vice versa. One technique to address this problem space is <strong><a href=""https://medium.com/@DoorDash/switchback-tests-and-randomized-experimentation-under-network-effects-at-doordash-f1d938ab7c2a"">Switchback experimentation</a></strong>. The core concept here is we switch back and forth between the ""A"" and ""B"" modes of the experiment in a certain region at alternating time periods instead of both running during the same time period. We then compare the customer experience and other key metrics between the two time buckets. We've tried this to good effect in some of our projects — it's a good tool to have in our experiments toolbelt.</p>"
Verifiable credentials,Assess,Techniques,TRUE,"<p>Credentials are everywhere in our lives and include passports, driver’s licenses and academic certificates. However, most digital credentials today are simple data records from information systems that are easy to modify and forge and often expose unnecessary information. In recent years, we've seen the continuous maturity of <strong>Verifiable Credentials</strong> solve this issue. The <a href=""https://www.w3.org/TR/vc-data-model/"">W3C standard</a> defines it in a way that is cryptographically secure, privacy respecting and machine verifiable. The model puts credential holders at the center, which is similar to our experience when using physical credentials: users can put their verifiable credentials in their own digital wallets and show them to anyone at any time without the permission of the credentials’ issuer. This decentralized approach also enables users to better manage their own information and selectively disclose certain information and greatly improves data privacy protection. For example, powered by zero-knowledge proof technology, you can construct a verifiable credential to prove that you are an adult without revealing your birthday. The community has developed many <a href=""https://www.w3.org/TR/vc-use-cases/"">use cases</a> around verifiable credentials. We've implemented our own COVID health certification with reference to the <a href=""https://www.covidcreds.com/"">COVID-19 Credentials Initiative (CCI)</a>. Although verifiable credentials don't rely on blockchain technology or <a href=""/radar/Techniques/decentralized-identity"">decentralized identity</a>, this technique often works with DID in practice and uses blockchain as a verifiable data registry. Many decentralized identity frameworks are also embedded with verifiable credentials.</p>"
Apollo Federation,Hold,Techniques,TRUE,"<p>When we first covered <a href=""/radar/languages-and-frameworks/graphql"">GraphQL</a> in the Radar, we cautioned that its misuse can lead to antipatterns which, in the long run, has more disadvantages than benefits. Nevertheless, we’ve seen an increasing interest in GraphQL among our teams because of its ability to <a href=""/radar/Techniques/graphql-for-server-side-resource-aggregation"">aggregate information from different resources</a>. This time we want to caution you about using <strong><a href=""https://www.apollographql.com/docs/apollo-server/federation/introduction/"">Apollo Federation</a></strong> and its  strong support for a single unified data graph for your company. Even though at first glance the idea of having ubiquitous concepts across the organization is tempting, we have to take into account previous similar attempts in the industry — such as <a href=""/radar/Techniques/master-data-management"">MDM</a> and canonical data model among others — that have exposed the pitfalls of this approach. The challenges can be significant, especially when the domain we find ourselves in is complex enough to create a unique unified model.</p>"
ESBs in API Gateway's clothing,Hold,Techniques,TRUE,"<p>We've long warned against <a href=""/radar/tools/esb"">centralized enterprise services buses</a> and defined <a href=""https://martinfowler.com/articles/microservices.html#SmartEndpointsAndDumbPipes"">""smart endpoints, dumb pipes""</a> as one of the core characteristics of a microservices architecture. Unfortunately, we're observing a pattern of traditional ESBs rebranding themselves, creating <strong>ESBs in API gateway's clothing</strong> that naturally encourage <a href=""/radar/platforms/overambitious-api-gateways"">overambitious API gateways</a>. Don't let the marketing fool you: regardless of what you call it, putting business logic (including orchestration and transformation) in a centralized tool creates architectural coupling, decreases transparency, and increases vendor lock-in with no clear upside. API gateways can still act as a useful abstraction for crosscutting concerns, but we believe the smarts should live in the APIs themselves.</p>"
Log aggregation for business analytics,Hold,Techniques,FALSE,"<p>Several years ago, a new generation of log aggregation platforms emerged that were capable of storing and searching over vast amounts of log data to uncover trends and insights in operational data. <a href=""/radar/tools/splunk"">Splunk</a> was the most prominent but by no means the only example of these tools. Because these platforms provide broad operational and security visibility across the entire estate of applications, administrators and developers have grown increasingly dependent on them. This enthusiasm spread as stakeholders discovered that they could use <strong>log aggregation for business analytics</strong>. However, business needs can quickly outstrip the flexibility and usability of these tools. Logs intended for technical observability are often inadequate to infer deep customer understanding. We prefer either to use tools and metrics designed for customer analytics or to take a more event-driven approach to observability where both business and operational events are collected and stored in a way they can be replayed and processed by more purpose-built tools.</p>"
Micro frontend anarchy,Hold,Techniques,TRUE,"<p>Since we originally introduced the term in 2016, <a href=""/radar/Techniques/micro-frontends"">micro frontends</a> have grown in popularity and achieved mainstream acceptance. But like any new technique with an easy-to-remember name, it has occasionally been misused and abused. Particularly concerning is the tendency to use this architecture as an excuse to mix a range of competing technologies, tools or frameworks in a single page, leading to <strong>micro frontend anarchy</strong>. A particularly egregious form of this syndrome is using multiple frontend frameworks — for example, <a href=""/radar/languages-and-frameworks/react-js"">React.js</a> and <a href=""/radar/languages-and-frameworks/angular"">Angular</a> — in the same ""single-page"" application. Although this might be technically possible, it is far from advisable when not part of a deliberate transition strategy. Other properties that should be consistent from team to team include the styling technique (e.g., <a href=""/radar/languages-and-frameworks/css-in-js"">CSS-in-JS</a> or <a href=""/radar/languages-and-frameworks/css-modules"">CSS modules</a>) and the means by which the individual components are integrated (e.g., iFrames or <a href=""/radar/platforms/web-components-standard"">web components</a>). Furthermore, organizations should decide whether to standardize on consistent approaches or to leave it up to their teams to decide on state management, data fetching, build tooling, analytics and a host of other choices in a micro frontend application.</p>"
Productionizing notebooks,Hold,Techniques,FALSE,"<p>Over the last few decades <a href=""https://en.wikipedia.org/wiki/Notebook_interface"">computational notebooks</a>, first introduced by <a href=""https://en.wikipedia.org/wiki/Wolfram_Mathematica"">Wolfram Mathematica</a>, have evolved to support scientific research, exploration and educational workflows. Naturally, in support of data science workflows and with the likes of <a href=""https://jupyter.org/"">Jupyter notebooks</a> and <a href=""https://docs.databricks.com/notebooks/index.html"">Databricks notebooks</a>, they've become a great companion by providing a simple and intuitive interactive computation environment for combining code to analyze data with rich text and visualization to tell a data story. Notebooks were designed to provide an ultimate medium for modern scientific communication and innovation. In recent years, however, we've seen a trend for notebooks to be the medium for running the type of production-quality code typically used to drive enterprise operations. We see notebook <a href=""https://databricks.com/blog/2017/10/30/continuous-integration-continuous-delivery-databricks.html"">platform providers advertising</a> the use of their exploratory notebooks in production. This is a case of good intentions — democratizing programming for data scientists — implemented poorly and at the cost of scalability, maintainability, resiliency and all the other qualities that a long-lived production code needs to support. We don't recommend <strong>productionizing notebooks</strong> and instead encourage empowering data scientists to build production-ready code with the right programming frameworks, thus simplifying the <a href=""/radar/Techniques/continuous-delivery-for-machine-learning-cd4ml"">continuous delivery</a> tooling and abstracting complexity away through end-to-end ML platforms.</p>"
Azure DevOps,Trial,Platforms,FALSE,"<p><strong><a href=""https://azure.microsoft.com/en-us/services/devops/"">Azure DevOps</a></strong> services contain a set of managed services, including hosted Git repos, CI/CD pipelines, automated testing tooling, backlog management tooling and artifact repository. We've seen our teams getting more experience in using this platform with good results, which means Azure DevOps is maturing. We particularly like its flexibility; it allows you to use the services you want even if they're from different providers. For instance, you could use an external Git repository while still using the Azure DevOps pipeline services. Our teams are especially excited about <a href=""https://azure.microsoft.com/en-us/services/devops/pipelines/"">Azure DevOps Pipelines</a>. Nevertheless, all the services offer a good developer experience that helps our teams deliver value.</p>"
Debezium,Trial,Platforms,FALSE,"<p><strong><a href=""https://debezium.io/"">Debezium</a></strong> is a <a href=""https://en.wikipedia.org/wiki/Change_data_capture"">change data capture (CDC)</a> platform that can stream database changes onto <a href=""/radar/tools/apache-kafka"">Kafka</a> topics. CDC is a popular technique with multiple use cases, including replicating data to other databases, feeding analytics systems, extracting microservices from monoliths and invalidating caches. Debezium reacts to changes in the database's log files and has CDC connectors for multiple databases, including Postgres, MySQL, Oracle and MongoDB. We're using Debezium in many projects, and it has worked very well for us.</p>"
Honeycomb,Trial,Platforms,FALSE,"<p><strong><a href=""https://www.honeycomb.io/"">Honeycomb</a></strong> is an observability service that ingests rich data from production systems and makes it manageable through dynamic sampling. Developers can log large amounts of rich events and later decide how to slice and correlate them. This interactive approach is useful when working with today's large distributed systems, because we've passed the point where we can reasonably anticipate which questions we might want to ask of production systems. The Honeycomb team is actively developing for a number of languages and frameworks with plugins now available for <a href=""/radar/languages-and-frameworks/go-language"">Go</a>, <a href=""/radar/platforms/node-js"">Node</a>, Java and Rails among others; other new features are being added at a rapid pace. The pricing model has also been simplified to make it more attractive. Our teams love it.</p>"
JupyterLab,Trial,Platforms,FALSE,"<p>Since introducing <strong><a href=""https://jupyterlab.readthedocs.io/en/stable/getting_started/overview.html"">JupyterLab</a></strong> in the Assess ring in our last issue, it has become the preferred web-based user interface for Project <a href=""/radar/tools/jupyter"">Jupyter</a> for many of our data practitioners. JupyterLab use is rapidly overtaking Jupyter Notebooks, which it will eventually replace. If you're still using Jupyter Notebooks, you should give JupyterLab a try. Its interactive environment is an evolution of Jupyter Notebook: it extends the original capabilities with drag-and-drop cells and tab autocompletion among other new features.</p>"
Amundsen,Assess,Platforms,TRUE,"<p>Data scientists spend a large part of their time on data discovery, which means tooling to help in this space is bound to generate some excitement. Although the <a href=""/radar/platforms/apache-atlas"">Apache Atlas</a> project has become the de facto tool for metadata management, data discovery is still not easily accomplished. Enter <strong><a href=""https://github.com/amundsen-io/amundsen"">Amundsen</a></strong>, which can be deployed in concert with Apache Atlas to provide a much nicer search interface for data discovery.</p>"
AWS Cloud Development Kit,Assess,Platforms,FALSE,"<p>For many of our teams, <a href=""/radar/tools/terraform"">Terraform</a> has become the default choice for defining cloud infrastructure. However, some of our teams have been experimenting with <strong><a href=""https://docs.aws.amazon.com/cdk/latest/guide/home.html"">AWS Cloud Development Kit</a></strong> (AWS CDK), and they like what they've seen so far. In particular, they like the use of first-class programming languages instead of configuration files which allows them to use existing tools, test approaches and skills. Like similar tools, care is still needed to ensure deployments remain easy to understand and maintain. It currently supports <a href=""/radar/languages-and-frameworks/typescript"">TypeScript</a>, JavaScript, Python, Java and C# and .NET. We'll continue to watch AWS CDK, especially since the AWS and HashiCorp teams recently launched a <a href=""https://aws.amazon.com/blogs/developer/introducing-the-cloud-development-kit-for-terraform-preview/"">preview for Cloud Development Kit for Terraform</a> to generate Terraform configurations and enable provisioning with the Terraform platform.</p>"
Backstage,Assess,Platforms,TRUE,"<p>Organizations are looking to support and streamline development environments through developer portals or platforms. As the number of tools and technologies increases, some form of standardization is becoming increasingly important for consistency so that developers are able to focus on innovation and product development instead of getting bogged down with reinventing the wheel. A centralized developer portal can offer easy discoverability of services and best practices. <strong><a href=""https://backstage.io/"">Backstage</a></strong> is an open-source platform for creating developer portals by Spotify. It is based upon software templates, unifying infrastructure tooling and consistent and centralized technical documentation. Its plugin architecture allows for extensibility and adaptability into an organization’s infrastructure ecosystem.</p>"
Dremio,Assess,Platforms,TRUE,"<p><strong><a href=""https://www.dremio.com/"">Dremio</a></strong> is a cloud data lake engine that powers interactive queries against cloud data lake storage. With Dremio, you don't have to manage data pipelines in order to extract and transform data into a separate data warehouse for predictive performance. Dremio creates virtual data sets from data ingested into a data lake and provides a uniform view to consumers. <a href=""/radar/platforms/presto"">Presto</a> popularized the technique of separating storage from the compute layer, and Dremio takes it further by improving performance and optimizing cost of operation.</p>"
DuckDB,Assess,Platforms,TRUE,"<p><strong><a href=""https://duckdb.org/"">DuckDB</a></strong> is an embedded, columnar database for data science and analytical workloads. Analysts spend significant time cleaning and visualizing data locally before scaling it to servers. Although databases have been around for decades, most of them are designed for client-server use cases and therefore not suitable for local interactive queries. To work around this limitation analysts usually end up using in-memory data-processing tools such as <a href=""https://pandas.pydata.org/"">Pandas</a> or <a href=""https://github.com/Rdatatable/data.table"">data.table</a>. Although these tools are effective, they do limit the scope of analysis to the volume of data that can fit in memory. We feel DuckDB neatly fills this gap in tooling with an embedded columnar engine that is optimized for analytics on local, larger-than-memory data sets.</p>"
K3s,Assess,Platforms,TRUE,"<p><strong><a href=""https://k3s.io/"">K3s</a></strong> is a lightweight <a href=""/radar/platforms/kubernetes"">Kubernetes</a> distribution built for IoT and edge computing. It's packaged as a single binary and has minimal to no OS dependencies, making it really easy to operate and use. It uses <a href=""https://docs.python.org/3/library/sqlite3.html"">sqlite3</a> as the default storage backend instead of <a href=""https://etcd.io/"">etcd</a>. It has a reduced memory footprint because it runs all relevant components in a single process. It also achieves a smaller binary by stripping out third-party storage drivers and cloud providers that are not relevant for the K3s use cases. For environments with constrained resources, K3s is a pretty good choice and worth considering.</p>"
Materialize,Assess,Platforms,TRUE,"<p><strong><a href=""https://materialize.io/"">Materialize</a></strong> is a streaming database that enables you to do incremental computation without complicated data pipelines. Just describe your computations via standard SQL views and connect Materialize to the data stream. The underlying <a href=""https://github.com/TimelyDataflow/differential-dataflow"">differential data flow</a> engine performs incremental computation to provide consistent and correct output with minimal latency. Unlike traditional databases, there are no restrictions in defining these materialized views and the computations are executed in real time.</p>"
Pulumi,Assess,Platforms,FALSE,"<p>We've seen interest in <strong><a href=""https://pulumi.io/"">Pulumi</a></strong> slowly but steadily rising. Pulumi fills a gaping hole in the infrastructure coding world where <a href=""/radar/tools/terraform"">Terraform</a> maintains a firm hold. While Terraform is a tried-and-true standby, its declarative nature suffers from inadequate abstraction facilities and limited testability. Terraform is adequate when the infrastructure is entirely static, but dynamic infrastructure definitions call for a real programming language. Pulumi distinguishes itself by allowing configurations to be written in <a href=""/radar/languages-and-frameworks/typescript"">TypeScript</a>/JavaScript, <a href=""/radar/languages-and-frameworks/python-3"">Python</a> and <a href=""/radar/languages-and-frameworks/go-language"">Go</a> — no markup language or templating required. Pulumi is tightly focused on cloud-native architectures — including containers, serverless functions and data services — and provides good support for <a href=""/radar/platforms/kubernetes"">Kubernetes</a>. Recently, <a href=""/radar/platforms/aws-cloud-development-kit"">AWS CDK</a> has mounted a challenge, but Pulumi remains the only cloud-neutral tool in this area. We're anticipating wider Pulumi adoption in the future and looking forward to a viable tool and knowledge ecosystem emerging to support it.</p>"
Tekton,Assess,Platforms,TRUE,"<p><strong><a href=""https://tekton.dev/"">Tekton</a></strong> is a young <a href=""/radar/platforms/kubernetes"">Kubernetes</a>-native platform for managing continuous integration and delivery (CI/CD) pipelines. It not only installs and runs on Kubernetes but also defines its CI/CD pipelines as Kubernetes <a href=""https://kubernetes.io/docs/concepts/extend-kubernetes/api-extension/custom-resources/"">custom resources</a>. This means the pipelines can now be controlled by native Kubernetes clients (CLI or APIs) and can take advantage of underlying resource management features such as rollbacks. The pipeline declaration format is flexible and allows defining workflows with conditions, parallel execution paths and handling final tasks to clean up among other features. As a result, Tekton can support complex and hybrid deployment workflows with rollbacks, canary release and more. Tekton is open source and also offered as a <a href=""https://cloud.google.com/tekton"">managed service by GCP</a>. Although the documentation has room for improvement and the community is growing, we've been using Tekton successfully for production workloads on <a href=""/radar/platforms/aws"">AWS</a>.</p>"
Trust over IP stack,Assess,Platforms,TRUE,"<p>Continuous challenges with how individuals and organizations establish trust digitally, over the internet, is giving rise to a new approach on how to prove identity, how to share and verify attributes needed to establish trust and how to securely transact. Our Radar features some of the foundational technologies such as <a href=""/radar/Techniques/decentralized-identity"">decentralized identity</a> and <a href=""/radar/Techniques/verifiable-credentials"">verifiable credentials</a> that enable this new era of digital trust.</p>

<p>However, such a global scale change won't be possible without a standardization of a technical governance stack that enables interoperability. The new <a href=""https://trustoverip.org/"">Trust over IP Foundation</a>, part of the Linux Foundation, has set out to do just that. Taking its inspiration from how TCP/IP standardization as the narrow waist of the internet has enabled interoperability across billions of devices, the group is defining a four-layer technical and governance <strong><a href=""https://github.com/hyperledger/aries-rfcs/blob/master/concepts/0289-toip-stack/README.md"">Trust over IP stack</a></strong>. The stack includes public utilities such as decentralized identifiers, decentralized identity <em>comms</em> to standardized protocols for agents such as digital wallets to communicate, data exchange protocols such as flows to issue and verify verifiable credentials, as well as the application ecosystems such as education, finance, healthcare, etc. If you're revisiting your identity systems and how you establish trust with your ecosystem, we suggest looking into ToIP stack and its supporting tooling, <a href=""https://www.hyperledger.org/projects/aries"">Hyperledger Aries</a>.</p>"
Node overload,Hold,Platforms,FALSE,"<p>Technologies, especially wildly popular ones, have a tendency to be overused. What we're seeing at the moment is <strong>Node overload</strong>, a tendency to use Node.js indiscriminately or for the wrong reasons. Among these, two stand out in our opinion. Firstly, we frequently hear that Node.js should be used so that all programming can be done in one programming language. Our view remains that <a href=""/radar/Techniques/polyglot-programming"">polyglot programming</a> is a better approach, and this still goes <a href=""/radar/languages-and-frameworks/javascript-as-a-first-class-language"">both ways</a>. Secondly, we often hear teams cite performance as a reason to choose Node.js. Although there are myriads of more or less sensible benchmarks, this perception is rooted in history. When Node.js became popular, it was the first major framework to embrace a nonblocking programming model which made it very efficient for IO-heavy tasks. (We mentioned this in our write-up of Node.js in 2012.) Due to its single-threaded nature, Node.js was never a good choice for compute-heavy workloads, though, and now that capable nonblocking frameworks also exist on other platforms — some with elegant, modern APIs — performance is no longer a reason to choose Node.js.</p>"
Airflow,Adopt,Tools,FALSE,"<p><strong><a href=""https://airflow.apache.org/"">Airflow</a></strong> remains our most widely used and favorite open-source workflow management tool for data-processing pipelines as directed acyclic graphs (DAGs). This is a growing space with open-source tools such as <a href=""https://github.com/spotify/luigi"">Luigi</a> and <a href=""https://github.com/argoproj/argo"">Argo</a> and vendor-specific tools such as <a href=""https://azure.microsoft.com/en-us/services/data-factory/"">Azure Data Factory</a> or <a href=""https://aws.amazon.com/datapipeline/"">AWS Data Pipeline</a>. However, Airflow differentiates itself with its programmatic definition of workflows over limited low-code configuration files, support for automated testing, open-source and multiplatform installation, rich set of integration points to the data ecosystem and large community support. In decentralized data architectures such as <a href=""/radar/Techniques/data-mesh"">data mesh</a>, however, Airflow currently falls short as a centralized workflow orchestration.</p>"
Bitrise,Adopt,Tools,FALSE,"<p><strong><a href=""https://www.bitrise.io/"">Bitrise</a></strong>, a domain-specific CD tool for mobile applications, continues to be a useful part of the mobile workflow, and teams really should be using it. Bitrise can build, test and deploy mobile applications all the way from developer laptop to app store publishing. It's easy to set up and provides a comprehensive set of prebuilt steps for most mobile development needs.</p>"
Dependabot,Adopt,Tools,FALSE,"<p>Among the available tools for keeping dependencies up to date, <strong><a href=""http://dependabot.com/"">Dependabot</a></strong> is a solid default choice in our opinion. Dependabot's integration with <a href=""/radar/tools/github"">GitHub</a> is smooth and automatically sends you pull requests to update your dependencies to their latest versions. It can be enabled at the organization level, so it's very easy for teams to receive these pull requests. If you're not using GitHub, you can still use the Dependabot libraries within your build pipeline. If you're interested in an alternative tool, also consider <a href=""https://github.com/renovatebot/renovate"">Renovate</a>, which supports a wider range of services, including <a href=""/radar/tools/gitlab"">GitLab</a>, Bitbucket and <a href=""/radar/platforms/azure-devops"">Azure DevOps</a>.</p>"
Helm,Adopt,Tools,FALSE,"<p><strong><a href=""http://helm.sh/"">Helm</a></strong> is a package manager for <a href=""/radar/platforms/kubernetes"">Kubernetes</a>. It comes with a repository of curated Kubernetes applications that are maintained in the official <a href=""https://github.com/helm/charts"">Charts repository</a>. Since we last talked about Helm, Helm 3 has been released, and the most significant change is the removal of Tiller, the server-side component of Helm 2. The benefit of a design without Tiller is that you can only make changes to the Kubernetes cluster from the client side, that is, you can only modify the cluster according to the permissions you have as a user of the Helm command. We've used Helm in a number of client projects and its dependency management, templating and hook mechanism has greatly simplified the application lifecycle management in Kubernetes.</p>"
Trivy,Adopt,Tools,FALSE,"<p>Build pipelines that create and deploy containers should include <a href=""/radar/Techniques/container-security-scanning"">container security scanning</a>. Our teams particularly like <strong><a href=""https://github.com/aquasecurity/trivy"">Trivy</a></strong>, a vulnerability scanner for containers. We've tried <a href=""https://github.com/quay/clair"">Clair</a> and <a href=""https://github.com/anchore/anchore-engine"">Anchore Engine</a> among other good tools in this field. Unlike Clair, Trivy doesn’t only check containers but also dependencies in the codebase. Also, because Trivy ships as a stand-alone binary, it's easier to set up and run the scan locally. Other benefits of Trivy are that it's open-source software and that it supports <a href=""/radar/Techniques/distroless-docker-images"">distroless containers</a>.</p>"
Bokeh,Trial,Tools,FALSE,"<p><strong><a href=""https://bokeh.org/"">Bokeh</a></strong> is one of the principal libraries in Python for creating scientific plots and data visualizations that render in the browser via JavaScript. Such tools, compared to desktop tools that create static images, make it easy to reuse code for exploratory work in web applications. Bokeh is particularly good for this. The library is mature and full-featured. What we like about Bokeh: it's great at keeping to its concern as a presentation layer tool and not trying to take on concerns such as data aggregation (see <a href=""http://ggplot.yhathq.com/"">ggplot</a>) or web app development (such as <a href=""https://shiny.rstudio.com/"">Shiny</a> or <a href=""/radar/tools/dash"">Dash</a>). This makes it a joy to use when separation of concerns is important to you. Bokeh does provide web UI widgets and can run in server mode, but you can take or leave these features as you see fit. Bokeh is flexible, and it doesn't make too many assumptions about how you'll use it nor does it have many dependencies (such as <a href=""https://pandas.pydata.org/"">pandas</a> or notebooks).</p>"
Concourse,Trial,Tools,FALSE,"<p>Implementing sustainable continuous delivery pipelines that can build and deploy production software across multiple environments requires a tool that treats build pipelines and artifacts as first-class citizens. When we first started assessing <strong><a href=""https://concourse-ci.org/"">Concourse</a></strong>, we liked its simple and flexible model, the principle of container-based builds and the fact that it forces you to define <a href=""/radar/Techniques/pipelines-as-code"">pipelines as code</a>. Since then, the usability has improved, and the simple model has stood the test of time. Many of our teams and clients have successfully been using Concourse for large pipeline setups over longer periods of time. We also often leverage Concourse's flexibility to run workers anywhere, for example, when hardware integration tests require a local setup.</p>"
Dash,Trial,Tools,TRUE,"<p>This edition of the Radar introduces several new tools for creating web applications that help end users visualize and interact with data. These are more than simple visualization libraries such as <a href=""/radar/tools/d3"">D3</a>. Instead, they reduce the effort necessary to build standalone analytic applications for manipulating existing data sets. <strong><a href=""https://plotly.com/dash/"">Dash</a></strong> from Plotly is gaining popularity among data scientists for creating richly functional analytics applications in Python. Dash augments Python data libraries much like <a href=""https://shiny.rstudio.com/"">Shiny</a> sits on top of R. These applications are sometimes referred to as dashboards, but the range of possible functionality is really much greater than the term implies. Dash is particularly suited to building scalable, production-ready applications, unlike <a href=""/radar/languages-and-frameworks/streamlit"">Streamlit</a>, another tool in this class. Consider using Dash when you need to present more sophisticated analyses to business users than a low- or no-code solution such as Tableau can provide.</p>"
jscodeshift,Trial,Tools,TRUE,"<p>Maintaining large-scale JavaScript codebases is never easy, but it's especially challenging when migrating breaking changes. IDEs with refactoring capabilities may help in simple scenarios. However, when your codebase is a widely dependent library, every time you make a breaking change you have to go through a series of client codebases to make the appropriate updates — which requires human oversight and needs to be done manually. <strong><a href=""https://github.com/facebook/jscodeshift"">jscodeshift</a></strong>, a toolkit to refactor JavaScript and <a href=""/radar/languages-and-frameworks/typescript"">TypeScript</a>, helps relieve this pain. It can parse your code to abstract syntax trees (AST) and provides an API to manipulate the tree with various transformations (e.g., adding, renaming and deleting properties from existing components) and then exports the tree as final source code. jscodeshift also comes with a simple unit testing utility, which can apply test-driven development for writing migration codemods. We've found jscodeshift to be quite helpful when maintaining <a href=""/radar/Techniques/design-systems"">design systems</a>.</p>"
Kustomize,Trial,Tools,TRUE,"<p><strong><a href=""https://kustomize.io/"">Kustomize</a></strong> is a tool to manage and customize Kubernetes manifest files. It allows you to select and patch your base Kubernetes resources before applying them to different environments and is now natively supported by <a href=""https://kubernetes.io/docs/reference/kubectl/overview/"">kubectl</a>. We like it because it helps keep your code <a href=""https://wiki.c2.com/?DontRepeatYourself"">DRY</a> and in contrast to <a href=""/radar/tools/helm"">Helm</a> (which is trying to do many things — package management, version management and so on), we find Kustomize follows the Unix philosophy: do one thing well and expect the output of every program to be input to another.</p>"
MLflow,Trial,Tools,TRUE,"<p><strong><a href=""https://mlflow.org/"">MLflow</a></strong> is an open-source tool for <a href=""/radar/tools/experiment-tracking-tools-for-machine-learning"">machine-learning experiment tracking</a> and lifecycle management. The workflow to develop and continuously evolve a machine-learning model includes a series of experiments (a collection of runs), tracking the performance of these experiments (a collection of metrics) and tracking and tweaking models (projects). MLflow facilitates this workflow nicely by supporting existing open standards and integrates well with many other tools in the ecosystem. <a href=""https://databricks.com/product/managed-mlflow"">MLflow as a managed service by Databricks</a> on the cloud, available in <a href=""/radar/platforms/aws"">AWS</a> and <a href=""/radar/platforms/azure"">Azure</a>, is rapidly maturing and we've used it successfully in our projects. We find MLflow a great tool for model management and tracking, supporting both UI-based and API-based interaction models. Our only growing concern is that MLflow is attempting to deliver too many conflating concerns as a single platform, such as model serving and scoring.</p>"
Pitest,Trial,Tools,FALSE,"<p>Traditional testing approaches focus on evaluating if our production code is doing what it's supposed to do. However, we could make mistakes in the testing code introducing incomplete or useless assertions that create a false sense of confidence. This is where mutation testing comes in; it assesses the quality of the tests themselves, finding corner cases that are hard to realize. Our teams have used <strong><a href=""http://pitest.org/"">Pitest</a></strong> for a while now, and we recommend its use in Java projects to measure the health of the test suite. In short, mutation testing introduces changes in the production code and executes the same tests a second time; if the tests are still green it means that the tests are not good and need to improve. When you’re using programming languages other than Java <a href=""/radar/tools/stryker"">Stryker</a> is a good choice in this space.</p>"
Sentry,Trial,Tools,FALSE,"<p><strong><a href=""https://sentry.io/"">Sentry</a></strong> is a cross-platform application monitoring tool with a focus on error reporting. Tools like Sentry distinguish themselves from traditional logging solutions such as the <a href=""https://www.elastic.co/elk-stack"">ELK Stack</a> in their focus on discovering, investigating and fixing errors. Sentry has been around for a while and supports several languages and frameworks. We've used Sentry in many projects, and it has been really useful in tracking errors, finding out if a commit actually fixed an issue and alerting us if an issue resurfaces due to a regression.</p>"
ShellCheck,Trial,Tools,TRUE,"<p>Even though tooling has vastly improved in the infrastructure space, writing a shell script may make sense in some cases. Of course, the syntax of shell scripts can only be described as arcane, and as we've less practice writing shell scripts these days, we've come to like <strong><a href=""https://www.shellcheck.net/"">ShellCheck</a></strong>, a linter for shell scripts. ShellCheck can be used from the command line, as part of a build or, even better, as an extension in many popular IDEs. The wiki contains a detailed description of several hundred issues that ShellCheck can detect, and most tools and IDEs provide a way to conveniently access the respective wiki page when an issue is found.</p>"
Stryker,Trial,Tools,TRUE,"<p><strong><a href=""https://stryker-mutator.io/"">Stryker</a></strong> is a relatively new entry in the mutation testing space. Similar to <a href=""/radar/tools/pitest"">Pitest</a>, Stryker lets you evaluate the quality of your tests. We've been using it quite successfully in JavaScript projects, but it also supports C# and Scala projects. Stryker is very user friendly and highly customizable, and we've been able to increase code coverage as well as confidence in the applications we're delivering for our clients.</p>"
Terragrunt,Trial,Tools,FALSE,"<p>We've used <a href=""/radar/tools/terraform"">Terraform</a> extensively to create and manage cloud infrastructure. In our experience with larger setups, where code is divided into modules that are included in different ways, teams eventually hit a wall of unavoidable repetition caused by a lack of flexibility. We've addressed this by using <strong><a href=""https://github.com/gruntwork-io/terragrunt"">Terragrunt</a></strong>, a thin wrapper for Terraform that implements the practices advocated by Yevgeniy Brikman’s <em><a href=""https://www.oreilly.com/library/view/terraform-up-and/9781491977071/"">Terraform: Up and Running</a></em>. We've found Terragrunt helpful because it encourages versioned modules and reusability for different environments. Lifecycle hooks are another useful feature providing additional flexibility. In terms of packaging, Terragrunt has the same limitations as Terraform: there is no proper way to define packages or dependencies between packages. As a workaround, you can use modules and specify a version associated with a Git tag.</p>"
tfsec,Trial,Tools,FALSE,"<p>Security is everyone's concern, and capturing risks early is always better than facing problems later on. In the <a href=""/radar/Techniques/infrastructure-as-code"">infrastructure as code</a> space — where <a href=""/radar/tools/terraform"">Terraform</a> has been an obvious choice to manage cloud environments — we now also have <strong><a href=""https://github.com/liamg/tfsec"">tfsec</a></strong>, a static analysis tool that scans Terraform templates to find potential security issues. Our teams have been using tfsec quite successfully. The tool is easy to set up and use, which makes it a great choice for any development team determined to mitigate security risks to prevent breaches before they happen. Its preset rules for different cloud providers, including <a href=""/radar/platforms/aws"">AWS</a> and <a href=""/radar/platforms/azure"">Azure</a>, compliment the benefits that tfsec brings to the teams that use Terraform.</p>"
Yarn,Trial,Tools,FALSE,"<p><strong><a href=""https://yarnpkg.com/"">Yarn</a></strong> continues to be the package manager of choice for many teams. We're excited about Yarn 2, a major new release with a long list of changes and improvements. In addition to usability tweaks and improvements in the area of workspaces, Yarn 2 introduces the concept of <em>zero-installs</em>, which allows developers to run a project directly after cloning it. However, Yarn 2 includes some breaking changes which makes the upgrade <a href=""https://yarnpkg.com/advanced/migration"">nontrivial</a>. It also defaults to <a href=""https://classic.yarnpkg.com/en/docs/pnp/"">plug'n'play</a> (PnP) environments and at the same time doesn't support React Native in PnP environments. Teams can, of course, opt out of PnP or stay on Yarn 1. They should be aware, though, that Yarn 1 is now in maintenance mode.</p>"
CML,Assess,Tools,TRUE,"<p>We've included <a href=""/radar/Techniques/continuous-delivery-for-machine-learning-cd4ml"">continuous delivery for machine learning</a> as a technique in previous Radars, and in this edition we want to highlight a promising new tool called <strong><a href=""https://cml.dev/"">Continuous Machine Learning</a> (or CML)</strong> from the people who made <a href=""/radar/tools/dvc"">DVC</a>. CML aims to bring the best engineering practices of CI and CD to AI and ML teams and can help to organize your MLOps infrastructure on top of a traditional software engineering stack, instead of creating separate AI platforms. We like that they've prioritized support for DVC and see this as a good sign for this burgeoning new tool.</p>"
Eleventy,Assess,Tools,TRUE,"<p>We've long liked the idea of using <a href=""/radar/Techniques/static-site-generators"">static site generators</a> to avoid complexity and improve performance, whenever the use case allows it. Although <strong><a href=""https://www.11ty.dev/"">Eleventy</a></strong> has been around for a few years, it's recently caught our attention as it's matured and previous favorites such as <a href=""/radar/languages-and-frameworks/gatsby-js"">Gatsby.js</a> displayed some scalability problems. Eleventy is quick to learn and easy to build sites with. We also like the ease with which you can create semantic (and therefore more accessible) markup with its templating and its simple and robust support for pagination.</p>"
Flagger,Assess,Tools,TRUE,"<p><a href=""/radar/Techniques/service-mesh"">Service meshes</a> and API gateways provide a convenient way to route traffic to a variety of microservices, all of which implement the same API interface. <strong><a href=""https://flagger.app/"">Flagger</a></strong> uses this feature to dynamically adjust the portion of traffic that is routed to a new version of a service. This is a common technique for <a href=""/radar/Techniques/1-canary"">canary releases</a> or blue/green deployment. Flagger works in conjunction with a variety of popular proxies (including Envoy and <a href=""/radar/tools/kong-api-gateway"">Kong</a>) to progressively ramp up requests to a service and report metrics on the load in order to provide fast feedback on a new release. We like that Flagger simplifies this valuable practice so that it can be more widely adopted. Although Flagger is sponsored by Weaveworks, it stands on its own with no obligation to use it in conjunction with Weaveworks' other tooling.</p>"
gossm,Assess,Tools,TRUE,"<p>When connecting to server instances on <a href=""/radar/platforms/aws"">AWS</a>, it is recommended to go through a bastion host instead of a direct connection. However, provisioning a bastion host just for that purpose can be frustrating, which is why <a href=""https://docs.aws.amazon.com/systems-manager/latest/userguide/session-manager.html"">AWS Systems Manager’s Session Manager</a> provides tunneling to more comfortably connect to your servers. <strong><a href=""https://github.com/gjbae1212/gossm"">gossm</a></strong> is an open-source CLI tool that makes the use of the Session Manager even more convenient. gossm lets you leverage the security provided by Session Manager and IAM policies from your terminal using tools such as <code>ssh</code> and <code>scp</code>. It also has some capabilities that the AWS CLI is missing, including server discovery and SSH integration.</p>"
Great Expectations,Assess,Tools,TRUE,"<p>With the rise of <a href=""/radar/Techniques/continuous-delivery-for-machine-learning-cd4ml"">CD4ML</a>, operational aspects of data engineering and data science have received more attention. Automated data governance is one aspect of this development. <strong><a href=""https://docs.greatexpectations.io/en/latest/"">Great Expectations</a></strong> is a framework that enables you to craft built-in controls that flag anomalies or quality issues in data pipelines. Just as unit tests run in a build pipeline, Great Expectations makes assertions during execution of a data pipeline. This is useful not only for implementing a sort of <a href=""https://en.wikipedia.org/wiki/Andon_(manufacturing)"">Andon</a> for data pipelines but also for ensuring that model-based algorithms remain within the operating range determined by their training data. Automated controls like these can help distribute and democratize data access and custodianship. Great Expectations also ships with a profiler tool to help understand the qualities of a particular data set and to set appropriate limits.</p>"
k6,Assess,Tools,TRUE,"<p>We're quite excited by <a href=""https://k6.io/""><strong>k6</strong></a>, a relatively new tool in the performance testing ecosystem with a heavy focus on developer experience. The k6 command line runner executes scripts written in JavaScript and allows you to configure the execution time and the number of virtual users. The CLI has several <a href=""https://k6.io/blog/how-to-control-a-live-k6-test"">advanced features</a> that let you see the current statistics before the test has finished executing, scale the number of virtual users beyond what was originally defined and even pause and resume a running test. The command line output provides a set of customizable metrics with transformers that let you visualize the results in <a href=""https://www.datadoghq.com/"">Datadog</a> and other observability tools. Adding <a href=""https://k6.io/docs/using-k6/checks"">checks</a> to your scripts is an easy way to integrate performance testing into your CI/CD pipeline. For accelerated performance testing, check out the commercial version, <a href=""https://k6.io/cloud"">k6 Cloud</a>, which provides cloud scaling and additional visualizations.</p>"
Katran,Assess,Tools,TRUE,"<p><strong><a href=""https://github.com/facebookincubator/katran"">Katran</a></strong> is a high-performance layer 4 load balancer. It's not for everyone, but if you need redundancy for layer 7 load balancers (such as <a href=""http://www.haproxy.org/"">HAProxy</a> or <a href=""https://www.nginx.com/"">NGINX</a>) or need to scale load balancers to two or more servers, then we recommend assessing Katran. We see Katran as a flexible and efficient choice over Techniques such as round-robin DNS over L7 load balancers or the IPVS Kernel model that network engineers usually adopt to solve similar challenges.</p>"
Kiali,Assess,Tools,TRUE,"<p>Given the increased use of <a href=""/radar/Techniques/service-mesh"">service mesh</a> to deploy collections of containerized microservices, we can expect to see tools emerge that automate and simplify the administrative tasks associated with this architectural style. <strong><a href=""https://kiali.io/"">Kiali</a></strong> is one such tool. Kiali provides a graphical user interface to observe and control networks of services deployed with <a href=""/radar/platforms/istio"">Istio</a>. We've found Kiali useful for visualizing the topology of services in a network and understanding the traffic routed between them. For example, when used in conjunction with <a href=""/radar/tools/flagger"">Flagger</a>, Kiali can display requests that have been routed to a canary service release. We particularly like Kiali's ability to artificially inject network faults into a service mesh to test resilience in the face of network interruptions. This practice is all too often ignored due to the complexity of configuring and running failure tests in a complex mesh of microservices.</p>"
LGTM,Assess,Tools,TRUE,"<p>Writing secure code is as important as ever, but it's only one of the many things developers have to prioritize. <strong><a href=""http://lgtm.com/"">LGTM</a></strong> provides both a safety net and a means to benefit from a knowledge base of secure coding practices. It is a static code analysis tool with a focus on security that is backed by a (partially open-source) catalog of secure coding rules. The rules are implemented as queries over your codebase in the <a href=""https://help.semmle.com/QL/learn-ql/index.html"">CodeQL</a> query language. It can be used to integrate white-box security checks into your CD pipelines for Java, Go, JavaScript, Python, C# and C/C++. LGTM and CodeQL are part of the <a href=""https://securitylab.github.com/"">Github Security Lab</a>.</p>"
Litmus,Assess,Tools,TRUE,"<p><strong><a href=""https://litmuschaos.io/"">Litmus</a></strong> is a chaos engineering tool with a low barrier to entry. It allows you to inject various error scenarios into your <a href=""/radar/platforms/kubernetes"">Kubernetes</a> cluster with minimal effort. We're particularly excited by the range of capabilities Litmus offers beyond your random pod kill, including simulating network, CPU, memory and I/O issues. Litmus also supports tailored experiments to simulate errors for <a href=""/radar/tools/apache-kafka"">Kafka</a> and Cassandra among other common services.</p>"
Opacus,Assess,Tools,TRUE,"<p>The concept of <a href=""/radar/Techniques/differential-privacy"">differential privacy</a> first appeared in the Radar in 2016. Although the problem of breaking privacy through systematic model inference queries was recognized at the time, it was largely a theoretical issue since remedies were few. The industry has lacked tools to prevent this from happening. <strong><a href=""https://github.com/pytorch/opacus"">Opacus</a></strong> is a new Python library that can be used in conjunction with <a href=""/radar/languages-and-frameworks/pytorch"">PyTorch</a> to help thwart one type of differential privacy attack. Although this is a promising development, finding the right model and data set to which it applies has been a challenge. The library is still quite new so we're looking forward to seeing how it'll be accepted going forward.</p>"
OSS Index,Assess,Tools,TRUE,"<p>It's important for a development team to identify whether the dependencies of their application have known vulnerabilities. <strong><a href=""https://ossindex.sonatype.org/"">OSS Index</a></strong> could be used to achieve this goal. OSS Index is a free catalog of open-source components and scanning tools designed to help developers identify vulnerabilities, understand risk and keep their software safe. Our teams are already integrating this index into pipelines via different languages, including <a href=""https://github.com/sonatype-nexus-community/auditjs"">AuditJS</a> and <a href=""https://github.com/sonatype-nexus-community/scan-gradle-plugin"">Gradle plugin</a>. The speed is fast, vulnerabilities are identified accurately and few false positives occur.</p>"
Playwright,Assess,Tools,TRUE,"<p>Web UI testing continues to be an active space. Some of the folks who built <a href=""/radar/languages-and-frameworks/puppeteer"">Puppeteer</a> have since moved on to Microsoft and are now applying their learnings to <strong><a href=""https://playwright.dev/"">Playwright</a></strong>, which allows you to write tests for Chromium and Firefox as well as WebKit, all through the same API. Playwright has gained some attention for its support of all the major browser engines, which it currently achieves by including patched versions of Firefox and Webkit. It remains to be seen how quickly other tools can catch up, with more and more support for the <a href=""https://chromedevtools.github.io/devtools-protocol/"">Chrome DevTools Protocol</a> as a common API for automating browsers.</p>"
pnpm,Assess,Tools,TRUE,"<p><strong><a href=""https://pnpm.js.org/"">pnpm</a></strong> is an up-and-coming package manager for <a href=""/radar/platforms/node-js"">Node.js</a> that we're looking at closely because of its higher speed and greater efficiency compared to other package managers. Dependencies are saved in a single place on the disk and are linked into the respective <code>node_modules</code> directories. pnpm also supports incremental optimization on file level, provides a solid API foundation to allow extension/customization and supports store server mode, which speeds up dependency download even more. If your organization has a large number of projects with the same dependencies, you may want to take a closer look at pnpm.</p>"
Sensei,Assess,Tools,TRUE,"<p><strong><a href=""https://securecodewarrior.com/sensei"">Sensei</a></strong> from Secure Code Warrior is a Java IDE plugin that makes it easy to create and distribute secure code quality guidelines. At ThoughtWorks we often advocate for ""tools over rules,"" that is, make it easy to do the right thing over applying checklist-like governance rules and procedures, and this tool fits this philosophy. Developers create recipes that can be easily shared with team members. These can be simple or complex and are implemented as queries targeting the Java AST. Examples include warnings for SQL injection, cryptographic weakness and many others. Another feature we like: Since it executes on code changes in the IDE, Sensei provides faster feedback than the more traditional static analysis tools.</p>"
Zola,Assess,Tools,TRUE,"<p><strong><a href=""https://www.getzola.org/"">Zola</a></strong> is a static site generator written in <a href=""/radar/languages-and-frameworks/rust"">Rust</a>. As such it comes as a single executable with no dependencies, is very fast and supports all the usual things you'd expect such as Sass, content in markdown and hot reloading. We've had success building static sites with Zola and appreciate how intuitive it is to use.</p>"
Arrow,Adopt,languages-and-frameworks,FALSE,"<p><strong><a href=""https://arrow-kt.io/"">Arrow</a></strong> is promoted as the functional companion for <a href=""/radar/languages-and-frameworks/kotlin"">Kotlin's standard library</a>. Indeed, the package of ready-to-use higher-level abstractions delivered by Arrow has proven so useful that our teams now consider Arrow a sensible default when working with Kotlin. Recently, in preparation for the 1.0 release, the Arrow team introduced several changes, including the addition of new modules but also some deprecations and removals.</p>"
jest-when,Adopt,languages-and-frameworks,FALSE,"<p><strong><a href=""https://www.npmjs.com/package/jest-when"">jest-when</a></strong> is a lightweight JavaScript library that complements <a href=""/radar/languages-and-frameworks/jest"">Jest</a> by matching mock function call arguments. Jest is a great tool for testing the stack; jest-when allows you to expect specific arguments for mock functions which enables you to write more robust unit tests of modules with many dependencies. It's easy to use and provides great support for multiple matchers, which is why our teams have made jest-when their default choice for mocking in this space.</p>"
Fastify,Trial,languages-and-frameworks,TRUE,"<p>In the case where implementation in <a href=""/radar/platforms/node-js"">Node.js</a> is necessary, we see that <a href=""https://www.fastify.io/""><strong>Fastify</strong></a> is an option that our teams are very happy with. This web framework offers ease in handling request-response validations, support for <a href=""/radar/languages-and-frameworks/typescript"">TypeScript</a> and a plugin ecosystem giving our teams an easier experience developing software. Although it's a good option in the Node.js ecosystem, we stand by our previous advice: don't fall into <a href=""/radar/platforms/node-overload"">Node overload</a> scenarios.</p>"
Immer,Trial,languages-and-frameworks,FALSE,"<p>With the increasing complexity of single-page JavaScript applications, managing state predictably is becoming more and more important. Immutability can help to ensure our applications behave consistently but unfortunately JavaScript doesn't offer built-in deeply immutable data structures (see the <a href=""https://github.com/tc39/proposal-record-tuple"">ES Record and Tuple proposal</a>). <strong><a href=""https://github.com/mweststrate/immer"">Immer</a></strong> — German for <em>always</em> — is a tiny package that lets you work with immutable state in a more convenient way. It's based on the copy-on-write mechanism, has a minimal API and operates on normal JavaScript objects and arrays. This means that data access is seamless and no large refactoring efforts are needed when introducing immutability to an existing codebase. Many of our teams now use it in their JavaScript codebases and prefer it to <a href=""/radar/languages-and-frameworks/immutable-js"">Immutable.js</a>, which is why we're moving it to Trial.</p>"
Redux,Trial,languages-and-frameworks,FALSE,"<p>We've decided to move <strong><a href=""http://redux.js.org/"">Redux</a></strong> back into the Trial ring to show that we no longer consider it the default approach for state management in <a href=""/radar/languages-and-frameworks/react-js"">React</a> applications. Our experience shows that Redux is still a valuable framework in many cases but compared to other approaches, it also leads to more verbose and harder-to-follow code. Throwing <a href=""https://redux-saga.js.org/"">Redux Sagas</a> into the mix usually compounds this issue. As an alternative, you can often use the features in recent versions of React to manage state effectively without an additional framework. However, we want to highlight that when you reach the point at which your simple state management solution starts to become complex, it might be worth reaching for Redux after all or perhaps even Facebook’s recently published <a href=""/radar/languages-and-frameworks/recoil"">Recoil</a>.</p>"
Rust,Trial,languages-and-frameworks,FALSE,"<p>The <strong><a href=""http://www.rust-lang.org/"">Rust</a></strong> programming language continues to grow in popularity and has been voted Stack Overflow's ""most loved"" language by developers five years in a row. We like it too. It's a fast, safe and expressive language that is increasing in utility as its ecosystem grows. For example, Rust is starting to be used for data science and machine learning and can give a <a href=""https://www.lpalmieri.com/posts/2019-12-01-taking-ml-to-production-with-rust-a-25x-speedup/"">significant performance boost</a>. Also, <a href=""/radar/platforms/materialize"">Materialize</a> is a streaming-oriented, low-latency database written in Rust.</p>"
single-spa,Trial,languages-and-frameworks,FALSE,"<p><strong><a href=""https://single-spa.js.org/"">single-spa</a></strong> is a JavaScript framework for bringing together multiple <a href=""/radar/Techniques/micro-frontends"">micro frontends</a> in a single front-end application. Although we advise against <a href=""/radar/Techniques/micro-frontend-anarchy"">micro frontend anarchy</a>, the use of micro frontends as an excuse to mix and match multiple frameworks, single-spa supports just that. We understand that there are legitimate scenarios such as upgrading to a new revision of a framework across multiple micro frontends where integration across multiple frameworks is necessary. single-spa has been a go-to framework for micro frontend integration for our teams, and they're finding it to work well with <a href=""https://github.com/systemjs/systemjs"">SystemJS</a> and managing different versions of a single dependency.</p>"
Strikt,Trial,languages-and-frameworks,TRUE,"<p>The <a href=""/radar/languages-and-frameworks/kotlin"">Kotlin</a> ecosystem keeps growing and more libraries are taking advantage of Kotlin language features to replace their Java alternatives. <strong><a href=""https://strikt.io/"">Strikt</a></strong> is an assertion library that allows you to write test assertions in a very fluent style. It uses Kotlin features such as blocks and lambdas to help make your tests less verbose while maintaining readability. Strikt also supports building custom assertions, which can make your tests more domain specific.</p>"
XState,Trial,languages-and-frameworks,FALSE,"<p>We've featured several state management libraries in the Radar before, but <strong><a href=""https://xstate.js.org/docs/"">XState</a></strong> takes a slightly different approach. It's a simple JavaScript and <a href=""/radar/languages-and-frameworks/typescript"">TypeScript</a> framework for creating finite state machines and visualizing them as state charts. It integrates with the more popular reactive JavaScript frameworks (<a href=""/radar/languages-and-frameworks/vue-js"">Vue.js</a>, <a href=""/radar/languages-and-frameworks/ember-js"">Ember.js</a>, <a href=""/radar/languages-and-frameworks/react-js"">React.js</a> and <a href=""https://rxjs.dev/"">RxJS</a>) and is based on the W3C standard for finite state machines. Another notable feature is the serialization of machine definitions. One thing that we've found helpful when creating finite state machines in other contexts (particularly when writing game logic) is the ability to visualize states and their possible transitions; we like that it's really easy to do this with XState's <a href=""https://xstate.js.org/viz/"">visualizer</a>.</p>"
Babylon.js,Assess,languages-and-frameworks,TRUE,"<p>When we wrote about <a href=""/radar/Techniques/vr-beyond-gaming"">VR beyond gaming</a> a few years ago we made no prediction on how quickly and to what extent VR solutions would be found in fields other than video gaming. In hindsight, we've certainly seen interest and adoption grow but the uptake has been slower than some of us anticipated. One reason could be tooling. <a href=""https://unity.com/"">Unity</a> and <a href=""https://www.unrealengine.com/"">Unreal</a> are two very mature and capable engines for developing VR applications. We also highlighted <a href=""/radar/platforms/godot"">Godot</a>. However, these engines are quite unlike what most web and enterprise teams are familiar with. As we continued exploring, we realized that web-based VR solutions have come a long way and we've had positive experience with <strong><a href=""https://www.babylonjs.com/"">Babylon.js</a></strong>. Written in TypeScript and rendering its applications in the browser, Babylon.js provides a familiar experience for many development teams. Additionally, Babylon.js is open-source software, mature and well-funded, which makes it even more attractive.</p>"
Blazor,Assess,languages-and-frameworks,TRUE,"<p>Although JavaScript and its ecosystem is dominant in the web UI development space, new opportunities are opening up with the emergence of <a href=""/radar/languages-and-frameworks/webassembly"">WebAssembly</a>. We see <strong><a href=""https://dotnet.microsoft.com/apps/aspnet/web-apps/blazor"">Blazor</a></strong> as an interesting option for building interactive web UIs using C#. We especially like this open-source framework because it allows running C# code in the browser on top of WebAssembly, leveraging the .NET Standard runtime and ecosystem as well as custom libraries developed in this programming language. Additionally, it can interoperate bidirectionally with JavaScript code in the browser if needed.</p>"
Flutter Driver,Assess,languages-and-frameworks,TRUE,"<p><strong><a href=""https://api.flutter.dev/flutter/flutter_driver/flutter_driver-library.html"">Flutter Driver</a></strong> is an integration testing library for <a href=""https://flutter.dev/"">Flutter</a> applications. With Flutter Driver you can instrument and drive the test suite on either real devices or emulators. Our teams continue to write unit and widget tests to ensure most of the business functionality in Flutter apps is implemented. However, for testing the actual user interaction, we're assessing Flutter Driver, and you should too.</p>"
HashiCorp Sentinel,Assess,languages-and-frameworks,TRUE,"<p>Although we're big advocates of defining <a href=""/radar/Techniques/security-policy-as-code"">security policy as code</a>, the tooling in this space has been fairly limited. If you're using HashiCorp products (such as <a href=""/radar/tools/terraform"">Terraform</a> or <a href=""/radar/tools/hashicorp-vault"">Vault</a>) and don't mind paying for the enterprise versions, you have the option of using <a href=""https://www.hashicorp.com/sentinel""><strong>HashiCorp Sentinel</strong></a>. Sentinel is, in effect, a complete programming language for defining and implementing context-based policy decisions. For example, in Terraform it can be used to test for policy violations before applying infrastructure changes. In Vault, Sentinel can be used to define fine-grained access control on the APIs. This approach has all the benefits of encapsulation, maintainability, readability and extensibility that high-level programming languages offer, creating an attractive alternative to traditional, declarative security policy. Sentinel is in the same class of tools as <a href=""/radar/tools/open-policy-agent-opa"">Open Policy Agent</a> but is proprietary, closed-source and only works with HashiCorp products.</p>"
Hermes,Assess,languages-and-frameworks,TRUE,"<p><strong><a href=""https://hermesengine.dev/"">Hermes</a></strong> is a JavaScript engine optimized for fast start-up of <a href=""/radar/languages-and-frameworks/react-native"">React Native</a> applications on Android. JavaScript engines such as <a href=""https://v8.dev/"">V8</a> have just-in-time (JIT) compilers that profile the code at run time to produce optimized instructions. Hermes, however, takes a different approach by compiling the JavaScript code ahead of time (AOT) into an optimized bytecode. As a result you get a smaller APK image size, lean memory consumption and faster startup time. We're carefully assessing Hermes in a few React Native apps and recommend you do the same.</p>"
io-ts,Assess,languages-and-frameworks,TRUE,"<p>We've been really enjoying using <a href=""/radar/languages-and-frameworks/typescript"">TypeScript</a> for a while now and love the safety that the strong typing provides. However, getting data into the bounds of the type system, from say a call to a back-end service, can lead to run-time errors. One library that helps solve this problem is <strong><a href=""https://gcanti.github.io/io-ts/"">io-ts</a></strong>. It bridges the gap between compile-time type-checking and run-time consumption of external data by providing encode and decode functions. It can also be used as a custom type guard. According to our teams, it's an elegant solution to a rascal of a problem.</p>"
Kedro,Assess,languages-and-frameworks,TRUE,"<p>In the past we've talked about the <a href=""/radar/tools/experiment-tracking-tools-for-machine-learning"">improving</a> <a href=""/radar/tools/dvc"">tooling</a> for applying <a href=""/radar/Techniques/continuous-delivery-for-machine-learning-cd4ml"">good engineering practices</a> in data science projects. <strong><a href=""https://github.com/quantumblacklabs/kedro"">Kedro</a></strong> is another good addition in this space. It's a development workflow framework for data science projects that brings a standardized approach to building production-ready data and machine-learning pipelines. We like the focus on software engineering practices and good design with its emphasis on test-driven development, modularity, versioning and good hygiene practices such as keeping credentials out of the codebase.</p>"
LitElement,Assess,languages-and-frameworks,TRUE,"<p>Steady progress has been made since we first wrote about <a href=""/radar/platforms/web-components-standard"">web components</a> in 2014. <strong><a href=""https://lit-element.polymer-project.org/"">LitElement</a></strong>, part of the <a href=""https://www.polymer-project.org/"">Polymer Project</a>, is a simple library that you can use to create lightweight web components. It's really just a base class that removes the need for a lot of the common boilerplate making writing web components a lot easier. We've had early success using it on projects and are excited to see the technology maturing.</p>"
Mock Service Worker,Assess,languages-and-frameworks,TRUE,"<p>Web applications, especially those written for internal use in enterprises, are usually written in two parts. The user interface and some business logic run in the web browser while business logic, authorization and persistence run on a server. These two halves normally communicate via JSON over HTTP. The endpoints shouldn't be mistaken for a real API; they're simply an implementation detail of an application that is split across two run-time environments. At the same time, they provide a valid seam to test the pieces individually. When testing the JavaScript part, the server side can be stubbed and mocked at the network level by a tool such as <a href=""/radar/tools/mountebank"">Mountebank</a>. An alternative approach is to intercept the requests in the browser. We like the approach taken by <strong><a href=""https://mswjs.io/"">Mock Service Worker</a></strong> because with service workers it uses an abstraction familiar to developers. This approach results in a simpler setup and faster test execution. However, because these tests don't test the actual network layer, you want to implement some end-to-end tests as part of a healthy test pyramid.</p>"
Recoil,Assess,languages-and-frameworks,TRUE,"<p>More and more teams using <a href=""/radar/languages-and-frameworks/react-js"">React</a> are reevaluating their options for state management, something we also mention in our reassessment of <a href=""/radar/languages-and-frameworks/redux"">Redux</a>. Now, Facebook — the creators of React — have published <strong><a href=""https://recoiljs.org/"">Recoil</a></strong>, a new framework for managing state, which came out of an internal application that had to deal with large amounts of data. Even though we currently do not have much practical experience with Recoil, we see its potential and promise. The API is simple and easy to learn; it feels like idiomatic React. Unlike other approaches, Recoil provides an efficient and flexible way to have state shared across an application: it supports dynamically created state by derived data and queries as well as app-wide state observation without impairing code splitting.</p>"
Snorkel,Assess,languages-and-frameworks,TRUE,"<p>Modern ML models are very complex and require massive amounts of labeled training data sets to learn from. <strong><a href=""https://www.snorkel.org/"">Snorkel</a></strong> started at the Stanford AI lab with the realization that manually labeling data is very expensive and often not feasible. Snorkel allows us to label training data programmatically via the creation of labeling functions. Snorkel employs supervised learning Techniques to assess the accuracies and correlations of these labeling functions, and then reweighs and combines their output labels, leading to high-quality training labels. The creators of Snorkel have since come out with a commercial platform called <a href=""https://www.snorkel.ai/"">Snorkel Flow</a>. While Snorkel itself is no longer actively developed, it's still significant for its ideas on the use of weakly supervised methods to label data.</p>"
Streamlit,Assess,languages-and-frameworks,TRUE,"<p><a href=""https://www.streamlit.io/""><strong>Streamlit</strong></a> is an open-source application framework in Python used by data scientists for building good-looking data visualization applications. Streamlit stands out from competitors such as Dash with its focus on rapid prototyping and support for a wide range of visualization libraries, including <a href=""https://plotly.com/"">Plotly</a> and <a href=""/radar/tools/bokeh"">Bokeh</a>. For data scientists who need quick showcases during the experimentation cycle, Streamlit is a solid choice. We're using it in a few projects and like how we can put together interactive visualizations with very little effort.</p>"
Svelte,Assess,languages-and-frameworks,TRUE,"<p>We continue to see new front-end JavaScript frameworks, and <strong><a href=""https://svelte.dev/"">Svelte</a></strong> stands out as a promising new component framework. Unlike other frameworks that leverage the virtual DOM, Svelte compiles your code into vanilla framework-less JavaScript code that surgically updates the DOM directly. However, it's only a component framework; if you're planning to build feature-rich applications, consider assessing <a href=""https://sapper.svelte.dev/"">Sapper</a> together with Svelte.</p>"
SWR,Assess,languages-and-frameworks,TRUE,"<p><strong><a href=""https://github.com/vercel/swr"">SWR</a></strong> is a <a href=""/radar/languages-and-frameworks/react-hooks"">React Hooks</a> library for fetching remote data. It implements the <a href=""https://tools.ietf.org/html/rfc5861"">stale-while-revalidate</a> HTTP caching strategy. SWR first returns data from cache (stale), then sends the fetch request (revalidate) and finally refreshes the values with the up-to-date response. Components receive a stream of data, first stale and then fresh, constantly and automatically. Our developers have had a good experience using SWR, dramatically improving the user experience with always having data on the screen. However, we caution teams to only use SWR caching strategy when appropriate for an application to return stale data. Note that <a href=""https://tools.ietf.org/html/rfc2616"">HTTP</a> requires that caches respond to a request with the most up-to-date response held that is appropriate to the request, and only in <em>carefully considered circumstances</em> is a stale response allowed to be returned.</p>"
Testing Library,Assess,languages-and-frameworks,TRUE,"<p><strong><a href=""https://testing-library.com/"">Testing Library</a></strong> is a family of packages for testing applications in numerous frameworks such as <a href=""/radar/languages-and-frameworks/react-js"">React</a>, <a href=""/radar/languages-and-frameworks/vue-js"">Vue</a>, <a href=""/radar/languages-and-frameworks/react-native"">React Native</a> and <a href=""/radar/languages-and-frameworks/angular"">Angular</a> among others. This set of libraries helps you test UI components in a user-centric way by encouraging you to test user behavior rather than implementation details, such as the presence of elements in the UI at a certain moment in time. One of the benefits of this mindset is more reliable tests, and this is what we call out as its main differentiator. We recommend you assess this family of libraries when testing your web applications in any framework. Although our direct experience is limited to <a href=""/radar/languages-and-frameworks/react-testing-library"">React Testing Library</a> and Angular Testing Library, we've been impressed with what we've seen.</p>"
